{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alekriley/alekriley.github.io/blob/master/importance_sampling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "GG9eH0OQSY3Q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Summary of Basic MonteCarlo Method\n",
        "We can approximate the function of a random variable using an average of the function computed over all samples.\n",
        "> $ E[f(x)]\\approx\\frac{1}{M}\\sum_{m=1}^M f(x^{(m)})$ for samples $x^{(1)},x^{(2)},\\dots,x^{(M)}$\n",
        "\n",
        "In the same way we can approximate the variance of $f(x)$.\n",
        ">  $ Var[f(x)]\\approx\\frac{1}{M-1}\\sum_{m=1}^M [f(x^{(m)})-u_f]^2$ for samples $x^{(1)},x^{(2)},\\dots,x^{(M)}$ with $u_f = \\hat E[f(x)]$."
      ]
    },
    {
      "metadata": {
        "id": "vHfu2BWKT3Pp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# The Idea of Importance Sampling\n",
        "\n",
        "Consider the expectation of any function of random variable.\n",
        "\n",
        "> $$E[f(x)]=\\int_x f(x)p(x)dx$$ \n",
        "\n",
        "where $p(x)$ is the probability density function corresponding to the random variable x. \n",
        "That is, we are taking the expectation with respect to $p(x)$. Notice however that we can easily do the following.\n",
        "> $$E[f(x)]=\\int_x q(x)f(x)\\frac{p(x)}{q(x)}dx=E_{q(x)}\\left[f(x)\\frac{p(x)}{q(x)}\\right]$$\n",
        "\n",
        "We can express the expectation of $f(x)$ with respect to a probability distribution $q(x)$. Typically we refer to $\\omega(x^{(i)})=\\frac{p(x^{(i)})}{q(x^{(i)})}$ as _importance weights_."
      ]
    },
    {
      "metadata": {
        "id": "oJxIKpCnWh7z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We might choose to do this for a few reasons:\n",
        "> 1. $q(x)$ is easier to sample from.\n",
        "> 2. We only know $p(x)$ up to a normalizing constant.\n",
        "> 3. To reduce the variance of a Monte Carlo Estimate.\n",
        "\n",
        "The primary focus of the rest of the notebook is on (3) ***variance reduction*** and specifically how _Importance Sampling_ helps us achieve this goal."
      ]
    },
    {
      "metadata": {
        "id": "b7uZdiPiZB8s",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Consider a theoretical example where \n",
        "> $$q(x) = \\frac{f(x)p(x)}{\\int_{x} f(x')p(x')dx'}$$ \n",
        "\n",
        "and $f(x)\\geq0$, then it is clear that the expectation of $f(x)\\frac{p(x)}{q(x)}$ is constant with respect to $q(x)$ and thus has ***zero variance***."
      ]
    },
    {
      "metadata": {
        "id": "PLlkmji7d4zu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "While it is clearly impossible to construct such a distribution in practice it nonetheless helps to show that by choosing a _good importance distribution_ $q(x)$ we can reduce the variance of our Monte Carlo Estimate.\n",
        "\n",
        "At this point it helps to think about what properties make a good _importance distribution_.\n",
        "> 1. It should be easy to sample from.\n",
        "> 2. Wherever $p(x)$ is non-zero $q(x)$ should be non-zero.\n",
        "> 3. It should be easy to compute $q(x)$ for all values of $x$.\n",
        "> 4. The closer it is to being proportional to $|f(x)|$ the better.\n",
        "\n",
        "A point to watch out for is that the tails of the distribution matter. If $q(x)$ approaches zero significantly faster than $p(x)$ than the importance weights will be very high (in theory tend towards infinity) and the variance of your estimator will most likely increase. \n"
      ]
    },
    {
      "metadata": {
        "id": "vMV98vSzvr5C",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Suppose we only have distribution up to a normalizing constant\n",
        "\n",
        "Calculate unnormalized importance weights $\\hat w^{(i)}=\\frac{\\hat p(x^{(i)})}{q(x^{(i)})}$ and normalize weights to find importance weights $$\\omega^{(i)}=\\frac{\\hat w^{(i)}}{\\sum_k \\hat p(x^{(k)})}$$\n",
        "\n",
        "This is possible because we assume that $p(x^{(i)})=\\frac{1}{Z}p(x^{(i)})$ then if we have adequate samples $Z =\\sum_k \\hat p(x^{(k)})$\n",
        "\n",
        "Let's do a few examples. First import necessary libraries."
      ]
    },
    {
      "metadata": {
        "id": "rPIwz4nE0FPv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Application: Estimating rare event probabilities\n",
        "\n",
        "Consider a density function $F(y)=\\int_{-\\infty}^y f(x)dx$.\n",
        "We can rewrite the integral as $$F(y)=\\int_{-\\infty}^\\infty [x\\leq y]f(x)dx$$ or in other words as an expectation... which has a corresponding Monte Carlo estimate\n",
        "> $$E[[x\\leq y]] \\approx \\frac{1}{M}\\sum_m [x^{(m)}\\leq y]$$\n",
        "\n",
        "where $[x^{(m)}\\leq y]$ is the iverson-bracket which has value one if the condition is true and zero otherwise. The conclusion is we can sample N points from our distribution and count all the ones which are less than y. If we knew nothing about probability this is what we would do!\n",
        "\n",
        "\\\\\n",
        "Let's reformulate the problem using _Importance Sampling_. \n",
        "> $$E[[x\\leq y]]\\approx \\frac{1}{M}\\sum_m [x^{(m)}\\leq y]\\frac{p(x^{(m)})}{q(x^{(m)})}$$\n",
        "\n",
        "Obviously we need to decide on our importance distribution. We want our distribution to be close to being proportional to our iverson-bracket. For our example we can use a transformed exponential distribution.\n",
        "> $$ Z = -(X-y) \\longrightarrow X = -Z+y$$ \n",
        "\n",
        "where $Z\\sim Exp(\\lambda)$. Why is the exponential a good choice? For all $x\\gt y$ the $[x\\leq y]$ will evaluate to zero. So even though $p(x)\\neq0$ for $x\\gt y$ the term will evaluate to zero. We will by design use **all** of our samples. We can expect a significantly better estimate."
      ]
    },
    {
      "metadata": {
        "id": "zZ22IujUx5f4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import scipy.stats as sts\n",
        "import seaborn as sne\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "sne.set_style('dark')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "O7qNZrkuu4rq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "1e27c637-5df5-438a-c9a7-a3e92f5dea36"
      },
      "cell_type": "code",
      "source": [
        "n_samples = 5000\n",
        "y = -3\n",
        "\n",
        "actual_p = sts.norm.cdf(y)\n",
        "\n",
        "####estimate using samples from normal distribution####\n",
        "x = np.random.randn(n_samples)\n",
        "estimated_p, std_p = np.mean((x<=y)*1), np.var((x<=y)*1)\n",
        "print(\"Actual probability of y less than or equal to {}: {}%\".format(y,np.round(100*actual_p,3)))\n",
        "print(\"Estimated probability of y less than or equal to {}: {}%\".format(y,np.round(100*estimated_p,3)))\n",
        "print(\"Variance of estimate: {}\".format(np.round(100**2*std_p,4)))"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Actual probability of y less than or equal to -3: 0.135%\n",
            "Estimated probability of y less than or equal to -3: 0.18%\n",
            "Variance of estimate: 17.9676\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tK452bnkwO32",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "96dc88e4-b9e2-420f-85f4-135b1be5f651"
      },
      "cell_type": "code",
      "source": [
        "z = sts.expon.rvs(size=n_samples)\n",
        "x = -z+y\n",
        "\n",
        "ws = sts.norm.pdf(x)/sts.expon.pdf(z)\n",
        "estimated_p, std_p = np.mean(ws),np.var(ws)\n",
        "print(\"Actual probability of y less than or equal to {}: {}%\".format(y,np.round(100*actual_p,3)))\n",
        "print(\"Estimated probability of y less than or equal to {}: {}%\".format(y,np.round(100*estimated_p,3)))\n",
        "print(\"Variance of estimate: {}\".format(np.round(100**2*std_p,3)))\n"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Actual probability of y less than or equal to -3: 0.135%\n",
            "Estimated probability of y less than or equal to -3: 0.135%\n",
            "Variance of estimate: 0.018\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5NUEvkIMrMFQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## General scheme to sample from an intractable distribution using Importance Sampling\n",
        "\n",
        "Sample $K$ points from the importance distribution. Calculate unnormalized importance weights $\\hat w^{(i)}=\\frac{\\hat p(x^{(i)})}{q(x^{(i)})}$ and normalize weights to find importance weights $$\\omega^{(i)}=\\frac{\\hat w^{(i)}}{\\sum_k \\hat p(x^{(k)})}$$\n",
        "\n",
        "Now we can sample N times with replacement from our vector $(x^{(1)},\\dots,x^{(K)})$ with probabilities $(\\omega^{(i)},\\dots,\\omega^{(K)})$ to approximate sampling from our intractable distribution. \n",
        "\n",
        "While this is a valid method of sampling from an intractable distribution we do not recommend this approach and would instead suggest some other method either the rejection method or MCMC both not covered here."
      ]
    }
  ]
}